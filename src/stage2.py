'''
This module contains methods used for the second stage of disambiguation.
in this stage, we use combined national hashes and the state-level results
to identify pairs of records that are candidates for comparison but haven't
already been clustered together (will be mostly cross-state pairs). Then,
We divide these records into approximately independent partitions each of
which can be independently analyzed by a child process. This analysis consists
of performing pairwise comparisons for each specified pair and deciding about
whether to merge their corresponding clusters.
In making this decision, cluster level statistics will be used. That is,
information regarding name/employer/occupation/etc. frequencies obtained
from stage1 clusters are used.
'''

from disambiguation.core import utils, Disambiguator, Project, Tokenizer
from disambiguation.core import hashes
import disambiguation.config as config
from disambiguation.core import Database



def get_candidate_pairs(num_pairs, state='USA', recompute=False):
    '''
    Get pairs of record ids that are similar according
    to the national (combined) hashes, but aren't already
    linked at the state level.
    @param state: the state whose hashes will be used.
    @param num_pairs: number of new records to select for comparison
    @return: list of tuples of record ids.
    '''

    # file that either already contains, or will contain a set of candidate
    # pairs of record ids together with a "weight"
    candidate_pairs_file = config.candidate_pairs_file_template % state

    # Make sure the file already exists
    if not recompute:
        if utils.os.path.exists(candidate_pairs_file):
            print "Candidate pairs file already exists. Skipping."
            return

    # Number of adjacent hashes to log
    B = 3

    # number of times to shuffle the list of hashes.
    num_shuffles = 40

    # list of candidate pairs to be compared.
    list_pairs = []

    # Get the full sorted edgelist from national hashes.
    filename = config.hashes_file_template % (state, 'Tokenizer')

    print "Starting get_edgelist_from_hashes_file()..."
    edgelist = hashes.get_edgelist_from_hashes_file(filename, state, B, num_shuffles, num_procs=3, num_pairs=num_pairs)
    # edgelist.reverse()
    print "Done with get_edgelist_from_hashes_file()"
    print "Done fetching new pairs."

    # Export edgelist to file
    with open(candidate_pairs_file,'w') as f:
        for edge in edgelist:
            f.write("%d %d %d\n" % (edge[0], edge[1], edge[2]))

        




def partition_records( num_partitions, state = "USA"):
    '''
    Partition the set of records appearing in the pairs identified
    by get_candidate_pairs into num_partitions subsets
    with minimal inter-set links, and export the edgelists within
    each sibset to a separate file for each subset. The list of record
    pairs is read from a file generated by get_candidate_pairs().
    
    @param num_partitions: number of partitions to divide list_record_pairs into.
    @status: only works if the giant component isn't "too" large. 
    '''
    
    
    # file that either already contains, or will contain a set of candidate
    # pairs of record ids together with a "weight"
    candidate_pairs_file = config.candidate_pairs_file_template % state
    

    ig = utils.igraph
    with open(candidate_pairs_file) as f:
        g = ig.Graph.Read_Ncol(f, names=True, weights="if_present", directed=False)
#         print g.vcount(), g.ecount()
    
  

    list_components = g.components().subgraphs()
    
    # partition the list of all connected components into a list of 
    # num_partition subsets such that the total number of nodes in each
    # partition is roughly constant.
    list_of_partitions = utils.partition_list_of_graphs(list_components, num_partitions)
    
    
    # Export the edgelist for each partition to a separate file.
    list_of_list_edges = [ [(g.vs[e.source]['name'], g.vs[e.target]['name'], str(int(e['weight']))) for g in partition for e in g.es] for partition in list_of_partitions]
    for counter,partition in enumerate(list_of_list_edges):
        with open(config.candidate_pairs_partitioned_file_template %(state, counter),'w') as f:
            for edge in partition:
                f.write("%s %s %s\n" % edge)
        
            

def disambiguate_subsets_multiproc(num_partitions, state="USA", num_procs = 12):
    '''
    Compare record pairs within each subset and save results.
    '''
    # List of filenames in which subsets of the edgelist are stored.
    list_filenames = [config.candidate_pairs_partitioned_file_template %(state, counter)\
                       for counter in range(num_partitions)]
    
    if num_procs == 1:
        for i,filename in enumerate(list_filenames):
            worker_disambiguate_subset_of_edgelist(filename)
    else:
        pool = utils.multiprocessing.Pool(num_procs)
        pool.map(worker_disambiguate_subset_of_edgelist,list_filenames)






def worker_disambiguate_subset_of_edgelist(filename):
    '''
    Disambiguate by comparing the record pairs in filename. To do this
    first extract the list of all record ids, generate a temp MySQL table
    with thos in it, then extract the full records from individual_contributions
    by joining it with the temp table.
    @note: the records need to be tokenized first, since record comparison relies
        on normalized names, etc.    
    @param filename: filename in which on partition of the edgelist is stored.
    '''
    list_tokenized_fields = ['NAME', 'CONTRIBUTOR_ZIP', 'ZIP_CODE', 'CONTRIBUTOR_STREET_1', 'CITY', 'STATE', 'EMPLOYER', 'OCCUPATION']
    list_auxiliary_fields = ['TRANSACTION_DT', 'TRANSACTION_AMT', 'CMTE_ID', 'ENTITY_TP', 'id']
    all_fields = list_tokenized_fields + list_auxiliary_fields
    ig = utils.igraph
    with open(filename) as f:
        print "Loading partition of edgelist."
        g = ig.Graph.Read_Ncol(f, names=True, weights="if_present", directed=False)
        list_record_ids = [int(v['name']) for v in g.vs]
         
        print "Getting list_of_pairs."
        list_of_pairs = [(int(g.vs[e.source]['name']),int(g.vs[e.target]['name'])) 
            for e in sorted(g.es, key = lambda e:e['weight'], reverse = True)]

        print "Retrieving records for this partition from database."
        retriever = Database.FecRetrieverByID(config.MySQL_tablename_all_records)
        retriever.retrieve(list_record_ids, all_fields)
        list_of_records = retriever.getRecords()
        

        project = Project.Project(1)

        tokenizer = Tokenizer.Tokenizer()
        project.tokenizer = tokenizer
        tokenizer.project = project
        tokenizer.setRecords(list_of_records)
        tokenizer.setTokenizedFields(list_tokenized_fields)
        
        
        print "Tokenizing records..."
        tokenizer.tokenize()
        list_of_records = tokenizer.getRecords()

        print "Instantiating Disambiguator."
        D = Disambiguator.Disambiguator(list_of_records, vector_dimension = None, matching_mode='thorough', num_procs=1)
        project.D = D
        D.project = project
        D.tokenizer = tokenizer
        
        print "Running D.disambiguate_list_of_pairs(list_of_pairs)"
        D.disambiguate_list_of_pairs(list_of_pairs)
    
    















