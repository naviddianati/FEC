'''
This module defines global variables used uniformly by various scripts.

@var data_path: the root path of the data tree structure.
@var dict_paths: a dict of various sub-directories of the data directory.
@var tokendata_file_template: template for fully qualified tokendata file. Takes two template parameters for state and Tokenizer class name.

@var vectors_file_template: This file contains a pickled dictionary \{r.id: vector for r in list_of_records\}.
@var hashes_file_template: This file contains the LSH hashes dictionary \{r.id: hash for r in list_of_records\}.
@var log_filename: name of the log file.
@var MySQL_table_individual_contributions: Name of table containing data directly imported 
from the officially released individual contributions table.
@var MySQL_table_state_combined: Name of table containing all records for given state, 
including the street address mined separately NOTE: THIS STRING IS A TEMPLATE. IT MUST BE
INSTANTIATED WITH A STATE NAME STRING.
@var MySQL_table_usa_combined: All <state>_combined tables concatenated.
@var MySQL_table_identities: Identities table. This table maps each record id to
an identity id.
@var MySQL_table_identities_adjacency: Identities adjacency table.
'''

import os

# Version of MySQL tables used 
# NOTE: this string cannot contain period '.' since
# it is used to generate MySQL table names.
FEC_version_data = '2'


# Version of the disambiguation results generated.
# This can be different from FEC_version_data.
FEC_version_disambiguation = '4'


# Truncation level for the affiliation networks 
# used in stage 1 disambiguation
# TODO: rewrite S1 code to use these constants.
percent_employers_S1 = 50
percent_occupations_S1 = 50


# Truncation level for the affiliation networks 
# used in stage 2 disambiguation
percent_employers_S2 = 50
percent_occupations_S2 = 50



# Global data path.
data_path = os.path.expanduser('~/data/FEC-v%s/' % FEC_version_disambiguation)



src_path = os.path.join(os.path.dirname(__file__), '')


dict_paths = {
    "data_path": data_path,
    "data_path_vectors" :  data_path + "vectors/",
    "data_path_hashes" : data_path + "hashes/",
    "data_path_tokendata" : data_path + "tokendata/",
    "data_path_normalized_attributes" : data_path + "normalized_atrributes/",
    "data_path_affiliations_employer" : data_path + "affiliations/employer/",
    "data_path_affiliations_occupation" : data_path + "affiliations/occupation/",
    "data_path_match_buffers" : data_path + "match_buffers/",
    "data_path_near_misses" : data_path + "near_misses/",
    "data_path_candidate_pairs" : data_path + "candidate_pairs/",
    "data_path_identities": data_path + "identities/",
    "data_path_exports": data_path + "exports/",
    "tmp_path": data_path + "tmp/",
    "results": data_path + "results/"
}

# Make sure the data paths exist
for url, directory in dict_paths.iteritems():
    if not os.path.exists(directory):
        os.makedirs(directory)





tokendata_file_template = dict_paths["data_path_tokendata"] + "%s-%s-tokendata.pickle"


# This file contains a pickled dictionary
# {r.id: vector for r in list_of_records}
vectors_file_template = dict_paths["data_path_vectors"] + "%s-%s-vectors.pickle"


# This file contains the LSH hashes dictionary
# {r.id: hash for r in list_of_records}
hashes_file_template = dict_paths["data_path_hashes"] + "%s-%s-hashes.pickle"


# File containing a dictionary of the normalized attributes
# of the records: {r.id: {attr_name:attr_value}}
normalized_attributes_file_template = dict_paths["data_path_normalized_attributes"] + "%s-normalized_attributes.pickle"


# File containing the edge list of related records
match_buffer_file_template = dict_paths["data_path_match_buffers"] + "%s-match_buffer.txt"


# File containing the results of all pairwise record comparisons
comparisons_file_template = dict_paths["data_path_near_misses"] + "%s-record_comparisons.json.txt"


# File with each line a json object documenting a near miss pair of records.
near_misses_file_template = dict_paths["data_path_near_misses"] + "%s-near_misses.json.txt"




# File containing the pairs of records found by hashes.get_edgelist_from_hashes_file()
# These are pairs that are candidates for comparison as judged from the hashes.
candidate_pairs_file_template = dict_paths["data_path_candidate_pairs"] + "%s-candidate_pairs.txt"
candidate_pairs_file_bootstrapping_template = dict_paths["data_path_candidate_pairs"] + "%s-candidate_pairs_bootstrapping.txt"

# file containing a subset of the edges in candidate_pairs_file_template. These partitions
# are generated by stage2.partition_records() and are meant to be loaded by child processes
# to be used for disambiguation.
candidate_pairs_partitioned_file_template = dict_paths["data_path_candidate_pairs"] + "%s-candidate_pairs_partition-%d.txt"



# File containing the pairs of S1 identities that must be compared since they contain records pairs
# found by hashes.get_edgelist_from_hashes_file() to be potential matches.
# These are pairs that are candidates for comparison as judged from the hashes.
candidate_S1_identity_pairs_file_template = dict_paths["data_path_candidate_pairs"] + "%s-S1_identity_candidate_pairs.txt"
candidate_S1_identity_pairs_file_bootstrapping_template = dict_paths["data_path_candidate_pairs"] + "%s-S1_identity_candidate_pairs_bootstrapping.txt"

# file containing a subset of the edges in candidate_S1_identity_pairs_file_template. These partitions
# are generated by stage2.partition_S1_identities() and are meant to be loaded by child processes
# to be used for disambiguation.
candidate_S1_identity_pairs_partitioned_file_template = dict_paths["data_path_candidate_pairs"] + "%s-candidate_S1_identity_pairs_partition-%d.txt"
candidate_S1_identity_pairs_partitioned_file_bootstrapping_template = dict_paths["data_path_candidate_pairs"] + "%s-candidate_S1_identity_pairs_partition_bootstrapping-%d.txt"


# file containing a subset of all record ids that need to be loaded. This is the subset
# consisting of all records that are associated with any of the S1 identities present inthe
# corresponding partition of identity pairs
candidate_list_records_partitioned_file_template = dict_paths["data_path_candidate_pairs"] + "%s-candidate_list_records_partition-%d.txt"
candidate_list_records_partitioned_file_bootstrapping_template = dict_paths["data_path_candidate_pairs"] + "%s-candidate_list_records_partition_bootstrapping-%d.txt"




# Affiliation graph file name templates
affiliation_employer_file_template = dict_paths["data_path_affiliations_employer"] + "%s-employer_graph.gml"
affiliation_occupation_file_template = dict_paths["data_path_affiliations_occupation"] + "%s-occupation_graph.gml"


# Post-stage1 affiliation graph file name templates
affiliation_poststage1_employer_file_template = dict_paths["data_path_affiliations_employer"] + "%s-poststage1_employer_graph.gml"
affiliation_poststage1_occupation_file_template = dict_paths["data_path_affiliations_occupation"] + "%s-poststage1_occupation_graph.gml"



# log file name
log_filename = data_path + "messages.log"

# String template for log messages.
# The three strings are datetime, msg_type and message
log_message_template = "[%s] (%s):\t%s\n"


# file containing all discovered related identities. Each
# line starts with a target identity and all other fields are
# related identities
related_identities_template = dict_paths['data_path_identities'] + "%s-related-identities.csv"


# file containing all discovered linked identities. Each
# line starts with a target identity and all other fields are
# linked identities
linked_identities_template = dict_paths['data_path_identities'] + "%s-linked-identities.csv"


# CSV file containin all records with an additional "identity" column as well
csv_exported_state_template = dict_paths['data_path_exports'] + "%s.csv"


# File where the results of S2 bootstrapping will be wrriten. 
# This data will be parsed and a machine learning algorithm
# will compute matching criteria from the data. The final S2
# disambiguation will use this matching criteria.
S2_bootstrap_results_file = dict_paths['data_path_identities'] + 'S2_bootstrap_results.txt' 


# File similar to S2_bootstrap_results_file, except it contains
# the identity comparison results for the actual disambiguation
# rather than bootstrapping.
S2_identity_comparison_results_file = dict_paths['data_path_identities'] + 'S2_identity_comparison_results.txt' 


# File containing the set of acceptable coordinates when
# the occupations are identical. Generated via bootstrapping.
filename_inds_o_4 =  dict_paths['data_path_identities'] + 'inds_o_4.json'

# File containing the set of acceptable coordinates when
# the employers are identical. Generated via bootstrapping.
filename_inds_e_4 =  dict_paths['data_path_identities'] + 'inds_e_4.json'

# File containing the set of acceptable coordinates when
# the occupations are linked. Generated via bootstrapping.
filename_inds_o_3 =  dict_paths['data_path_identities'] + 'inds_o_3.json'

# File containing the set of acceptable coordinates when
# the employers are linked. Generated via bootstrapping.
filename_inds_e_3 =  dict_paths['data_path_identities'] + 'inds_e_3.json'

# File containing the set of acceptable coordinates when
# the occupations are identical but BAD. Generated via bootstrapping.
filename_inds_o_2 =  dict_paths['data_path_identities'] + 'inds_o_2.json'

# File containing the set of acceptable coordinates when
# the employers are identical but BAD. Generated via bootstrapping.
filename_inds_e_2 =  dict_paths['data_path_identities'] + 'inds_e_2.json'






# MySQL

# User name for the MySQL client
MySQL_username = 'navid'


# A file containing the password for the
# MySQL client.
# NOTE: set proper permissions to protect this
# file.
MySQL_password_file = '/nfs/home/navid/.security/MySQL_pswd'

# password for the MySQL client
with open(MySQL_password_file) as f: 
    MySQL_password = f.read().strip()



# Table containing data directly imported from
# the officially released individual contributions
# table.
MySQL_table_individual_contributions = 'individual_contributions_v%s' % FEC_version_data

# Table containing all records for given state, 
# including the street address mined separately
# NOTE: THIS STRING IS A TEMPLATE. IT MUST BE INSTANTIATED
# WITH A STATE NAME STRING
MySQL_table_state_combined = '%%s_combined_v%s' % FEC_version_data

MySQL_table_state_full = '%%s_full_v%s' % FEC_version_data

MySQL_table_state_addresses = '%%s_addresses_v%s' % FEC_version_data

# All <state>_combined tables concatenated.
MySQL_table_usa_combined = 'usa_combined_v%s' % FEC_version_data

# Identities table. This table maps each record id to
# an identity id.
MySQL_table_identities = 'identities_v%s' % FEC_version_disambiguation

# Identities adjacency table.
MySQL_table_identities_adjacency = 'identities_adjacency_v%s' % FEC_version_disambiguation








print MySQL_table_individual_contributions
print MySQL_table_state_combined % 'newyork'
print MySQL_table_usa_combined 
print MySQL_table_identities 
print MySQL_table_identities_adjacency 




#Log all the variables in L{config} into a text file in the
#root data directory.
with open(data_path + 'config.log', 'w') as f:
    variables = {x:y for x,y in locals().iteritems()}
    for key,value in sorted(variables.items()):
        f.write('[%s] : %s\n\n' % (key, str(value)))




