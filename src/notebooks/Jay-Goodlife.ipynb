{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual_contributions_v2\n",
      "newyork_combined_v2\n",
      "usa_combined_v2\n",
      "identities_v5\n",
      "identities_adjacency_v5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from disambiguation.core import Database\n",
    "from disambiguation.core import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'identities_v5' exists.\n",
      "Table 'identities_adjacency_v5' exists.\n",
      "Table 'linked_identities_v5' exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'identities_v5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idm = Database.IdentityManager('USA')\n",
    "idm.table_name_identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recnum',\n",
       " 'clustera',\n",
       " 'clusterchar',\n",
       " 'cmte_id',\n",
       " 'ammendment',\n",
       " 'rpt_tp',\n",
       " 'retiringdebt',\n",
       " 'microfilmloc',\n",
       " 'transtype',\n",
       " 'contbr_nm',\n",
       " 'contbr_city',\n",
       " 'contbr_st',\n",
       " 'contbr_zip',\n",
       " 'contbr_occupation',\n",
       " 'month',\n",
       " 'day',\n",
       " 'century',\n",
       " 'year',\n",
       " 'contb_receipt_dt',\n",
       " 'otherid',\n",
       " 'fecrecordnum',\n",
       " 'negativeamount',\n",
       " 'char_137_decoded',\n",
       " 'amt',\n",
       " 'extracttype',\n",
       " 'state',\n",
       " 'submission',\n",
       " 'filingperiod',\n",
       " 'cluster',\n",
       " 'cmte_nm',\n",
       " 'cmte_treasurer_nm',\n",
       " 'cmte_st1',\n",
       " 'cmte_st2',\n",
       " 'cmte_city',\n",
       " 'cmte_st',\n",
       " 'cmte_zip',\n",
       " 'cmte_designation',\n",
       " 'cmte_type',\n",
       " 'cmte_party',\n",
       " 'cmte_filing_freq',\n",
       " 'cmte_interest_grp',\n",
       " 'cmte_connect_org_nm',\n",
       " 'cmte_candidate_id']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4382\n",
      "328\n"
     ]
    }
   ],
   "source": [
    "state = 'NY'\n",
    "homedir = '/nfs/home/navid/data/FEC/jaygoodliffe/'\n",
    "filename = homedir + '{state}_smautoclustzip.dta'.format(state = state.lower())\n",
    "data = pd.read_stata(filename)\n",
    "data.to_csv(homedir + '{state}_smautoclustzip.csv'.format(state = state.lower()), sep='|')\n",
    "print len(data)\n",
    "\n",
    "print len(data['cluster'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daterange = ' TRANSACTION_DT BETWEEN \"2001-01-01\" AND \"2004-12-30\" ;'\n",
    "\n",
    "def get_next_cluster(data):\n",
    "    '''\n",
    "    This generator returns a list of records from the \n",
    "    provided DataFrame. The list will correspond to one\n",
    "    of the clusters in the data and the generator will\n",
    "    continues until all clusters are returned.\n",
    "    '''\n",
    "    for cluster in  sorted(data['cluster'].unique()):\n",
    "        yield data[data['cluster'] == cluster]\n",
    "\n",
    "        \n",
    "def find_match(row, retriever, verbose = False):\n",
    "    '''\n",
    "    Receive a row of the Goodliffe data and return a list of\n",
    "    matching records from our own database.\n",
    "    @param row: a row from jg data.\n",
    "    @return: list of matched FEC records.\n",
    "    '''\n",
    "    list_tokenized_fields = ['NAME', 'ZIP_CODE', 'CONTRIBUTOR_STREET_1', 'CITY', 'STATE', 'EMPLOYER', 'OCCUPATION', 'IMAGE_NUM']\n",
    "    list_auxiliary_fields = ['TRANSACTION_DT', 'TRANSACTION_AMT', 'CMTE_ID', 'ENTITY_TP', 'id']\n",
    "    all_fields = list_tokenized_fields + list_auxiliary_fields\n",
    "    all_fields_query = ','.join(all_fields)\n",
    "    retriever.query_fields = all_fields\n",
    "    \n",
    "    jg_name = row['contbr_nm'].upper()\n",
    "    jg_cmte_id = row['cmte_id'].upper()\n",
    "    jg_city = row['contbr_city'].upper()\n",
    "    jg_state = row['contbr_st'].upper()\n",
    "    jg_zipcode = str(row['contbr_zip'])[:5]\n",
    "    jg_microfilm = row['microfilmloc']\n",
    "    jg_amount = int(row['amt'])\n",
    "    jg_year = str(row['year']).rjust(2,'0')\n",
    "    jg_month = str(row['month']).rjust(2,'0')\n",
    "    jg_day = str(row['day']).rjust(2,'0')\n",
    "    jg_century = row['century']\n",
    "    \n",
    "    # This is another cluster column which I'm not sure about.\n",
    "    # Assuming that this is their algorithm's clustering \n",
    "    jg_cluster = row['clustera']\n",
    "    \n",
    "    jg_date = '{century}{year}{month}{day}'.format(century = jg_century,\n",
    "                                                    year = jg_year,\n",
    "                                                    month = jg_month,\n",
    "                                                    day = jg_day)\n",
    "    \n",
    "    \n",
    "    \n",
    "    query = 'SELECT {fields} from usa_combined_v2 where IMAGE_NUM=\"{imagenum}\" AND {daterange}'.format(imagenum=jg_microfilm,\n",
    "                                                                                  fields=all_fields_query, daterange = daterange)\n",
    "\n",
    "    retriever.retrieve(query=query)\n",
    "    list_records = retriever.getRecords()\n",
    "    list_records_new = []\n",
    "    \n",
    "    for r in list_records:\n",
    "        matched = True\n",
    "    \n",
    "        \n",
    "        if int(r['TRANSACTION_AMT']) != jg_amount: matched = False\n",
    "        if r['TRANSACTION_DT'] != jg_date: matched = False\n",
    "        if r['CITY'] != jg_city: matched = False\n",
    "        if r['STATE'] != jg_state: matched = False\n",
    "        if r['CMTE_ID'] != jg_cmte_id: matched = False\n",
    "        if r['NAME'] != jg_name: matched = False\n",
    "        if r['ZIP_CODE'][:5] != jg_zipcode: matched = False\n",
    "        \n",
    "        if verbose and not matched:\n",
    "            print \"== \" + matched + \" \" + \"=\"*50\n",
    "            print \"AMNT:: \", int(r['TRANSACTION_AMT']) , jg_amount\n",
    "            print \"DATE:: \", r['TRANSACTION_DT'], jg_date\n",
    "            print \"CITY:: \", r['CITY'], jg_city\n",
    "            print \"STATE:: \", r['STATE'], jg_state\n",
    "            print \"=\"*50\n",
    "        \n",
    "        if matched: list_records_new.append(r)\n",
    "    return list_records_new\n",
    "        \n",
    "    \n",
    "def get_records(set_rids, retriever):\n",
    "    '''\n",
    "    Retrieve the records with rids from a given\n",
    "    set.\n",
    "    '''\n",
    "    if not set_rids:\n",
    "        return []\n",
    "    list_tokenized_fields = ['NAME', 'ZIP_CODE', 'CONTRIBUTOR_STREET_1', 'CITY', 'STATE', 'EMPLOYER', 'OCCUPATION', 'IMAGE_NUM']\n",
    "    list_auxiliary_fields = ['TRANSACTION_DT', 'TRANSACTION_AMT', 'CMTE_ID', 'ENTITY_TP', 'id']\n",
    "    all_fields = list_tokenized_fields + list_auxiliary_fields\n",
    "    all_fields_query = ','.join(all_fields)\n",
    "    retriever.query_fields = all_fields\n",
    "    \n",
    "    list_rids_str = '({})'.format(','.join([str(rid) for rid in set_rids]))\n",
    "    \n",
    "    query = 'SELECT {fields} from usa_combined_v2 where id in {list_rids_str} AND {daterange}'.format(fields=all_fields_query,\n",
    "                                                                                                      daterange = daterange,\n",
    "                                                                                                     list_rids_str = list_rids_str)\n",
    "\n",
    "    retriever.retrieve(query=query)\n",
    "    list_records = retriever.getRecords()\n",
    "    return list_records\n",
    "\n",
    "\n",
    "\n",
    "def get_compound_identity(rid, idm):\n",
    "    '''\n",
    "    Return the compound identity of rid by concatenating\n",
    "    the identities of all linked identities.\n",
    "    '''\n",
    "    identity = idm.get_identity(rid)\n",
    "    if identity is None:\n",
    "        return '', []\n",
    "    linked_identities = idm.get_linked_identities(identity)\n",
    "    \n",
    "    all_identities = linked_identities+[identity]\n",
    "    compound_identity = \"|\".join(sorted(all_identities)) \n",
    "    return compound_identity, all_identities\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def  get_all_related_fec_records(set_rids, idm):\n",
    "    '''Get all FEC rids in the same cluster \n",
    "    as the matched records.\n",
    "    @return: dict mapping rid to compound identity.\n",
    "    '''\n",
    "    dict_identities = {}\n",
    "    set_identities = set()\n",
    "    for rid in set_rids:\n",
    "        compound_identity, all_identities =  get_compound_identity(rid, idm)\n",
    "        \n",
    "        set_identities.update(set(all_identities))\n",
    "        dict_identities[rid] = compound_identity\n",
    "    for identity in set_identities:\n",
    "        list_rids = idm.get_ids(identity)\n",
    "        \n",
    "        for rid in list_rids:\n",
    "            if rid not in dict_identities:\n",
    "                compound_identity, all_identities =  get_compound_identity(rid, idm)\n",
    "                dict_identities[rid] = compound_identity\n",
    "    return dict_identities\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def process_cluster(batch_index, clustera, retriever, idm):\n",
    "    '''\n",
    "    perform matching for all records in the cluster and\n",
    "    return the full results.\n",
    "    '''\n",
    "    outdir = '/nfs/home/navid/data/FEC/jaygoodliffe/results/'\n",
    "    counter_nomatch = 0\n",
    "    counter_duplicate = 0\n",
    "    list_matches_data = []\n",
    "    list_columns = ['batch_index', 'jg_recnum', 'jg_cluster', 'jg_clustera', 'id', 'identity' ]\n",
    "    \n",
    "    # id to [(recnum,clustera), ...]. This allows us to find all records\n",
    "    # matching a row, and then find all other records we clustered\n",
    "    # together but which don't match any row.\n",
    "    dict_matches = {}\n",
    "    print \"Cluster \", batch_index\n",
    "\n",
    "#     print len(clustera)\n",
    "    for j, row in clustera.iterrows():\n",
    "        list_matches = find_match(row, retriever)\n",
    "        \n",
    "        if not list_matches:\n",
    "#             print \"O: No match found\"\n",
    "            counter_nomatch += 1\n",
    "            dict_matches[(row['recnum'], row['cluster'], row['clustera'])] = [None]\n",
    "            continue\n",
    "            \n",
    "#         print len(list_matches)\n",
    "        \n",
    "        # If there's only one match, accept\n",
    "        if len(list_matches) == 1:\n",
    "            record = list_matches[0]\n",
    "            dict_matches[(row['recnum'], row['cluster'], row['clustera'])] = [record.id]\n",
    "        else:\n",
    "#             print counter_duplicate , \" more than one match: \",  len(list_matches)\n",
    "            dict_matches[(row['recnum'], row['cluster'], row['clustera'])] = [record.id for record in list_matches]\n",
    "            counter_duplicate += 1\n",
    "\n",
    "    # At this point, we have all the results in \n",
    "    # one data structures:\n",
    "    # dict_matches maps (row['recnum'], row['clustera']) to a list of r_ids\n",
    "    set_identities = set([idm.get_identity(rid) \\\n",
    "                      for jg_pair, list_rids in dict_matches.iteritems() \\\n",
    "                          for rid in list_rids])\n",
    "    # all the FEC records matched\n",
    "    set_rids = set([rid for jg_pair, list_rids in dict_matches.iteritems() \\\n",
    "                          for rid in list_rids])\n",
    "    \n",
    "    # Get all FEC records in the same cluster as the matched records.\n",
    "    dict_all_fec_records = get_all_related_fec_records(set_rids, idm)\n",
    "\n",
    "    # Set of the FEC records that didn't exist in the row but we grouped together\n",
    "    # with some that were.\n",
    "    set_new_rids = set(dict_all_fec_records.keys()).difference(set_rids)\n",
    "    \n",
    "    list_new_records = get_records(set_new_rids, retriever)\n",
    "    \n",
    "    \n",
    "    list_matches_data = [[batch_index, key[0], key[1], key[2], rid, dict_all_fec_records[rid]]\\\n",
    "                        for key, list_rids in dict_matches.iteritems()\\\n",
    "                            for rid in list_rids]\n",
    "    list_matches_data += [[batch_index, '', '', '', record.id, dict_all_fec_records[record.id]]\\\n",
    "                         for record in list_new_records]\n",
    "    print \"new ids: \", len(set_new_rids)\n",
    "#     list_matches_data.append([record.id, identity, row['recnum'], row['clustera']])\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(list_matches_data, columns = list_columns)\n",
    "    df_full = pd.merge(df,clustera, left_on='jg_recnum', right_on = 'recnum', how='left')\n",
    "    outfile = outdir + 'jg_match_num_{}.csv'.format(batch_index)\n",
    "    df_full.sort(['jg_cluster','identity'],inplace=True)\n",
    "#     df_full['batch_id'] = batch_id\n",
    "    n = len(df_full)\n",
    "    m = len(df_full.columns)\n",
    "    spacer = pd.DataFrame([['    ' for i in range(m)] for j in range(5)] , columns = df_full.columns)\n",
    "    df_full = pd.concat([df_full,spacer])\n",
    "\n",
    "\n",
    "\n",
    "#  break   df_full.to_csv(outfile)\n",
    "#     df_full.to_html(outfile+'.html')pd.concat([df_full,\n",
    "    return df_full\n",
    "\n",
    "            \n",
    "\n",
    "retriever = Database.FecRetriever(table_name = 'usa_combined_v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-65cd9c70bb4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/nfs/home/navid/data/FEC/jaygoodliffe/results/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclustera\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_next_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#     if batch_index > 20: break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclustera\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretriever\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-111-b987a327bce1>\u001b[0m in \u001b[0;36mget_next_cluster\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcontinues\u001b[0m \u001b[0muntil\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclusters\u001b[0m \u001b[0mare\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     '''\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cluster'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cluster'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas-0.14.0-py2.7-linux-x86_64.egg/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1684\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas-0.14.0-py2.7-linux-x86_64.egg/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1691\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionaility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas-0.14.0-py2.7-linux-x86_64.egg/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas-0.14.0-py2.7-linux-x86_64.egg/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2536\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2537\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2538\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2539\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas-0.14.0-py2.7-linux-x86_64.egg/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[0mloc\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0munique\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly\u001b[0m \u001b[0mslice\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m         \"\"\"\n\u001b[1;32m-> 1156\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mindex.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3353)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mindex.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3233)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mhashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:11148)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mhashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:11101)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cluster'"
     ]
    }
   ],
   "source": [
    "list_dfs = []\n",
    "outdir = '/nfs/home/navid/data/FEC/jaygoodliffe/results/'\n",
    "\n",
    "for batch_index, clustera in enumerate(get_next_cluster(data)):\n",
    "#     if batch_index > 20: break\n",
    "    df = process_cluster(batch_index, clustera, retriever, idm)\n",
    "    list_dfs.append(df)\n",
    "df = pd.concat(list_dfs)\n",
    "df.to_csv(outdir+'jg_{state}_all_comparisons_date_restricted.csv'.format(state = state))\n",
    "print \"Saved to file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'NY-466378'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idm.get_identity(3110336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'CT-22864', u'CT-22867', u'NY-95941', u'NY-95974', u'NY-95978']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idm.get_linked_identities('CT-22870')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the full comparison CSV files and generate statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sizes_of_my_clusters(df):\n",
    "    '''\n",
    "    Receve a dataframe of records both in jay ana navid,\n",
    "    and find out how many navid clusters it's split into.\n",
    "    Return a sorted (descending) list of their sizes\n",
    "    '''\n",
    "    a = df.groupby(['identity'])\n",
    "    return sorted([len(x) for x in a.groups.values()], reverse = True)\n",
    "    \n",
    "\n",
    "def get_stats_1(df):\n",
    "    '''\n",
    "    Receive a dataframe for a given cluster and compute\n",
    "    match statistics.\n",
    "    '''\n",
    "    df_only_in_navid = df[df['id'].isnull()]\n",
    "    df_only_in_jay = df[df['jg_recnum'].isnull()]\n",
    "    df_in_both = df[df['jg_recnum'].notnull() & df['id'].notnull()]\n",
    "    \n",
    "    list_sizes_navid_clusters = get_sizes_of_my_clusters(df_in_both)\n",
    "    \n",
    "    \n",
    "    \n",
    "    num_only_in_navid = len(df_only_in_navid)\n",
    "    num_only_in_jay = len(df_only_in_jay)\n",
    "    num_in_both = len(df_in_both)\n",
    "    \n",
    "    \n",
    "    if len(df) != num_only_in_navid + num_only_in_jay + num_in_both: \n",
    "        raise Exception(\"numbers don't add up\")\n",
    "        \n",
    "    # Among the records we have in common, how close\n",
    "    # are our two clusterings?\n",
    "    num_distinct_identities = len(df_in_both['identity'].unique())\n",
    "    \n",
    "    # ratio of records found only in jay to \n",
    "    # records found in both.\n",
    "    if num_in_both == 0:\n",
    "        ratio_only_in_jay =  1000\n",
    "        ratio_only_in_navid =  1000\n",
    "    else:\n",
    "        ratio_only_in_jay =  num_only_in_jay / float(num_in_both)\n",
    "        ratio_only_in_navid =  num_only_in_navid / float(num_in_both)\n",
    "    \n",
    "    return[num_only_in_navid, num_only_in_jay, num_in_both, ratio_only_in_jay, ratio_only_in_navid, num_distinct_identities]\n",
    "\n",
    "\n",
    "def get_stats_2(df):\n",
    "    '''\n",
    "    Receive a dataframe for a given cluster and compute\n",
    "    match statistics.\n",
    "    '''\n",
    "    df_only_in_navid = df[df['id'].isnull()]\n",
    "    df_only_in_jay = df[df['jg_recnum'].isnull()]\n",
    "    df_in_both = df[df['jg_recnum'].notnull() & df['id'].notnull()]\n",
    "    \n",
    "    list_sizes_navid_clusters = get_sizes_of_my_clusters(df_in_both)\n",
    "    \n",
    "    if not list_sizes_navid_clusters: return None\n",
    "    \n",
    "    # ratio of the size of the larges navid cluster\n",
    "    # to the total size of the cluster.\n",
    "    ratio_largest_navid = list_sizes_navid_clusters[0]/float(sum(list_sizes_navid_clusters))\n",
    "    \n",
    "    return ratio_largest_navid\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/home/navid/data/FEC/jaygoodliffe/results/jg_NY_all_comparisons_date_restricted.csv\n"
     ]
    }
   ],
   "source": [
    "state = 'NY'\n",
    "\n",
    "outdir = '/nfs/home/navid/data/FEC/jaygoodliffe/results/'\n",
    "\n",
    "filename = outdir+'jg_{state}_all_comparisons_date_restricted.csv'.format(state=state)\n",
    "stats_filename = outdir+'jg_{state}_all_results_date_restricted.csv'.format(state=state)\n",
    "print filename\n",
    "\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats, version 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/home/navid/data/FEC/jaygoodliffe/results/jg_NY_all_comparisons_date_restricted.csv\n",
      "[7, 1, 1]\n",
      "   num_only_in_navid  num_only_in_jay  num_in_both  ratio_only_in_jay  \\\n",
      "0                  5               64            9           7.111111   \n",
      "\n",
      "   ratio_only_in_navid  num_distinct_identities  \n",
      "0             0.555556                        3  \n"
     ]
    }
   ],
   "source": [
    "columns = ['num_only_in_navid', 'num_only_in_jay', 'num_in_both', 'ratio_only_in_jay', 'ratio_only_in_navid', 'num_distinct_identities']\n",
    "list_rows = []\n",
    "\n",
    "for i in range(500):\n",
    "    a = data[data['batch_index'] == str(i)]\n",
    "#     print a.ix[:,:'clustera']\n",
    "    if a.empty: break\n",
    "    list_rows.append(get_stats_1(a))\n",
    "    break\n",
    "\n",
    "results = pd.DataFrame(list_rows,columns= columns)\n",
    "results.to_csv(stats_filename)\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Stats, version 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       recall\n",
      "0    0.777778\n",
      "1    0.571429\n",
      "2    0.935484\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.000000\n",
      "6    0.666667\n",
      "7    0.555556\n",
      "8    0.500000\n",
      "9    1.000000\n",
      "10   0.894737\n",
      "11   0.500000\n",
      "12   1.000000\n",
      "13   0.666667\n",
      "14   0.954545\n",
      "15   1.000000\n",
      "16   0.952381\n",
      "17   0.800000\n",
      "18   0.900000\n",
      "19   0.980392\n",
      "20   0.800000\n",
      "21   0.714286\n",
      "22   0.785714\n",
      "23   1.000000\n",
      "24   1.000000\n",
      "25   0.857143\n",
      "26   0.800000\n",
      "27   0.909091\n",
      "28   1.000000\n",
      "29   1.000000\n",
      "..        ...\n",
      "298  1.000000\n",
      "299  1.000000\n",
      "300  1.000000\n",
      "301  1.000000\n",
      "302  1.000000\n",
      "303  1.000000\n",
      "304  1.000000\n",
      "305  1.000000\n",
      "306  1.000000\n",
      "307  1.000000\n",
      "308  1.000000\n",
      "309  1.000000\n",
      "310  1.000000\n",
      "311  1.000000\n",
      "312  1.000000\n",
      "313  1.000000\n",
      "314  1.000000\n",
      "315  1.000000\n",
      "316  1.000000\n",
      "317  1.000000\n",
      "318  1.000000\n",
      "319  1.000000\n",
      "320  1.000000\n",
      "321  0.923077\n",
      "322  0.500000\n",
      "323  0.500000\n",
      "324  1.000000\n",
      "325  1.000000\n",
      "326  1.000000\n",
      "327  1.000000\n",
      "\n",
      "[328 rows x 1 columns]\n",
      "recall    0.91753\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "list_rows = []\n",
    "columns = ['recall']\n",
    "for i in range(500):\n",
    "    a = data[data['batch_index'] == str(i)]\n",
    "    if a.empty: break\n",
    "    list_rows.append(get_stats_2(a))\n",
    "#     break\n",
    "\n",
    "results = pd.DataFrame(list_rows,columns= columns)\n",
    "results.to_csv(stats_filename)\n",
    "print results\n",
    "print results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We measured the following quantity: for each jg handcoded cluster, we find all records in our database matching a record in that cluster. We then look at how we clustered those records. A perfect agreement would mean we grouped all those records into one cluster.  However, in some cases, we group those records into multiple clusters. We take the size of the largest of those, $m$, and divide it by the total number of records in the jg cluster we were able to match, $n$. \n",
    "\n",
    "## A measure of our recall given their ground truth is then $m/n$, or the percentage of mutual records in that cluster that were \"correctly\" grouped together by our algorithm.\n",
    "\n",
    "## For the state of NY, the average of this value over all 328 clusters is 0.92\n",
    "## For the state of UT,  the average of this value over all 368 clusters  is 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'num_only_in_navid', u'num_only_in_jay', u'num_in_both', u'ratio_only_in_jay', u'ratio_only_in_navid', u'num_distinct_identities'], dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44915254237\n",
      "1.24254157475\n",
      "0.249111345028\n"
     ]
    }
   ],
   "source": [
    "print results['num_distinct_identities'].mean()\n",
    "\n",
    "tmp = results['ratio_only_in_jay']\n",
    "print tmp[tmp != 1000].mean()\n",
    "\n",
    "tmp = results['ratio_only_in_navid']\n",
    "print tmp[tmp != 1000].mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
